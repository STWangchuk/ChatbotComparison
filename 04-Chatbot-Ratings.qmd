---
title: "Chatbot Ratings"
format: pdf
editor: visual
---

## Introduction

Chatbot Arena (lmarena.ai) is "an open-source platform for evaluating AI through human preference, developed by researchers at UC Berkeley [SkyLab](https://sky.cs.berkeley.edu/) and [LMSYS](https://lmsys.org/)." On the website <https://lmarena.ai/>, users can converse with two LLMs on any topic they choose. Users can then vote on which response they prefer (Model A or Model B) or can choose "Tie" or "Both bad" if they don't have a preference for either model. The LLMs are initially anonymous, but are revealed to the user after they vote.

## Data

The developers of this site provide data [here](https://huggingface.co/datasets/lmarena-ai/arena-human-preference-140k) on 140K conversations with pairwise human preferences. You can find this data in [this shared folder](https://yaleedu-my.sharepoint.com/:f:/g/personal/brian_macdonald_yale_edu/Eqo7cptmHAZAjNMv2F1dCj4BhZzs17e4hkfxbH8tMxLnwQ?e=Oo1DNN). There are 7 `.parquet` files. There is a README file in the GitHub repo that gives details about the data.

Each row is a matchup between two chat bots (e.g. Gemini 2.5 Pro vs Claude Sonnet 4), and the columns indicate a conversation ID, the names of the two models, who the user selected as the winner of the matchup, a common query that the two models were given, the category of the prompt (creative writing, math, code, etc.) and other information. See the README file for more details.

## Question

Which chatbot(s) appears to perform best in the eyes of the judges/users? In a matchup between any two models Model A and Model B, what are the outcome probabilities? How do these change if you focus on specific categories (e.g. writing, math, code, etc.) of prompts? Can you think of any reason the data may not be reliable, or any way this rating system could be "gamed"?

## Submission

Submit a PDF report with your code and analysis to Gradescope.







Cleaning the data below

```{r}
library(dplyr)
library(tidyr)
library(purrr)
library(tibble)
library(arrow) 
library(readr) 

Clean_parquet <- function(data) {
  clean_data <- data %>%
    select(
      -c(conversation_a, conversation_b, timestamp, evaluation_session_id, full_conversation),
      -matches("conv_meta")
    )
  return(clean_data)
}

flatten_tibble <- function(df, sep = "_") {
  while (any(sapply(df, tibble::is_tibble))) {
    df <- df %>%
      unnest_wider(where(tibble::is_tibble), names_sep = sep)
  }
  return(df)
}

parquet_folder <- "data"
parquet_files <- list.files(parquet_folder, pattern = "\\.parquet$", full.names = TRUE)

all_data <- map_dfr(parquet_files, function(file) {
  message("Processing: ", file)
  data <- read_parquet(file)
  data <- Clean_parquet(data)
  data <- flatten_tibble(data)
  return(data)
})

write_csv(all_data, "combined_clean_flat.csv")
```

```{r}
final_data <- read.csv("combined_clean_flat.csv")
final_data <- final_data %>%
  mutate(model_a_orig = model_a, model_b_orig = model_b) %>%
  pivot_longer(
    cols = c(model_a, model_b),
    names_to = "model_type",
    values_to = "model"
  ) %>%
  mutate(
    model_outcome = case_when(
      winner == model_type ~ "won",
      winner %in% c("tie", "both_bad") ~ winner,
      TRUE ~ "lost"
    ),
    opponent = if_else(model_type == "model_a", model_b_orig, model_a_orig)
  ) %>%
  select(-model_a_orig, -model_b_orig, - winner, -id)

final_data <- final_data[seq(1, nrow(final_data), by = 2), ]
final_data <- (filter(final_data, !(model == opponent)))
```


```{r}
model_perspective <- final_data %>%
  rename(model_name = model) %>%
  select(model_name, model_outcome)

opponent_perspective <- final_data %>%
  rename(model_name = opponent) %>%
  mutate(model_outcome = case_when(
    model_outcome == "won" ~ "lost",
    model_outcome == "lost" ~ "won",
    model_outcome == "tie" ~ "tie",
    model_outcome == "both_bad" ~ "both_bad"
  )) %>%
  select(model_name, model_outcome)

all_games <- bind_rows(model_perspective, opponent_perspective)

win_rates <- all_games %>%
  group_by(model_name) %>%
  summarise(
    total_games = n(),
    wins = sum(model_outcome == "won") + sum(model_outcome == "tie"),
    losses = sum(model_outcome == "lost") + sum(model_outcome == "both_bad"),
    win_rate = (sum(model_outcome == "won") + sum(model_outcome == "tie")) / n()
  ) %>%
  arrange(desc(win_rate))

print(win_rates)

```

```{r}
# Number of losses for each model. Given by the sum of the following:
# 1. You are the model, and model_outcome is "lost"
# 2. You are the opponent, and model_outcome is "won"
# 3. You are either, and the value is a "both_bad"

model_won <- filter(final_data, model_outcome == "won")
is_opponent_and_won <- table(model_won$opponent)

model_both_bad <- filter(final_data, model_outcome == "both_bad")
is_model_and_both_bad <- table(model_both_bad$model)
is_opponent_and_both_bad <- table(model_both_bad$opponent)

model_lost <- filter(final_data, model_outcome == "lost")
is_model_and_lost <- table(model_lost$model)

loss_table <- is_opponent_and_won + is_model_and_both_bad + is_opponent_and_both_bad + is_model_and_lost
```